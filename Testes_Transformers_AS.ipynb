{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111ba891",
   "metadata": {},
   "source": [
    "# Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c19eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9eedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab37520",
   "metadata": {},
   "source": [
    "# Import's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a279fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiago/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-07-16 12:08:33.665373: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-16 12:08:33.665466: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485662e3",
   "metadata": {},
   "source": [
    "# Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35393d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = './'\n",
    "dataset_path = \"Dataset/\"\n",
    "remove_dataset = 'concatenated.csv' \n",
    "file_dataset_facebook = 'facebook/'\n",
    "file_dataset_twitter = 'twitter/'\n",
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe0df5",
   "metadata": {},
   "source": [
    "## Baixando o Modelo Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28237122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-16 12:08:39.711230: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-16 12:08:39.711638: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-16 12:08:39.711700: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Thiaguinho-PC): /proc/driver/nvidia/version does not exist\n",
      "2022-07-16 12:08:39.717464: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-16 12:08:40.827626: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 768006144 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a784cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_as = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4ff46",
   "metadata": {},
   "source": [
    "## Lendo os datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665773bd",
   "metadata": {},
   "source": [
    "### Dataset's \"Brazilian Portuguese Sentiment Analysis Datasets\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pegando os dataset's \"Brazilian Portuguese Sentiment Analysis Datasets\" (https://www.kaggle.com/datasets/fredericods/ptbr-sentiment-analysis-datasets?resource=download)\n",
    "list_file = [file for file in os.listdir(DIR) if file.endswith('.csv')] ## pegando os arquivos \".csv\"\n",
    "list_file.remove(dataset_concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851230a5",
   "metadata": {},
   "source": [
    "##### Erro\n",
    "Erro: Quando passamos uma lista de datasets para serem carregados, ocorre um erro, porém se carregar um por um esse erro não ocorre, e se carregar usando o dataset concatenado o erro acontece.\n",
    "Visto que o dataset do \"buscape.csv\", em sua coluna \"original_index\" o tipo é uma \"string\" enquando nos demais dataset's essa mesma coluna é \"int64\", causando um conflito no merge realizado pela a biblioteca dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a49040",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## dataset_BPSA = load_dataset(\"csv\", data_files = list_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e48635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Metodo funcional\n",
    "dataset_BPSA = []\n",
    "for file in list_file:\n",
    "    dataset_BPSA.append(load_dataset(\"csv\", data_files = file))\n",
    "    dataset_BPSA[-1]['dataset_name'] = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890373c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(dataset_BPSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9efecce",
   "metadata": {},
   "source": [
    "#### Analisando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d1e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quantidade de informações: \" + str(sum([len(x['train']) for x in dataset_BPSA])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d3918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_BPSA[0]['train'][142]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97953e79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(dataset_BPSA)):\n",
    "    print(dataset_BPSA[i]['train'].features)\n",
    "    print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4207ee91",
   "metadata": {},
   "source": [
    "##### Avaliando tamanho dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5bf914",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in dataset_BPSA:\n",
    "    cont = 0\n",
    "    for data in dataset['train']:\n",
    "        resultado = 0\n",
    "        if data['review_text_processed'] is not None:\n",
    "            resultado = len(data['review_text_processed'].split(' '))\n",
    "        if resultado >= 512:\n",
    "            cont += 1\n",
    "    print(str(dataset['dataset_name']) + \" - \" + str(cont))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7475e58a",
   "metadata": {},
   "source": [
    "##### Teste manual com pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96253b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Texto: \" + str(dataset_BPSA[0]['train'][0]['review_text']))\n",
    "print(pipeline_as(dataset_BPSA[0]['train'][0]['review_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Texto: \" + str(dataset_BPSA[0]['train'][9]['review_text_processed']))\n",
    "print(pipeline_as(dataset_BPSA[0]['train'][9]['review_text_processed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Texto: \" + str(dataset_BPSA[0]['train'][2]['review_text_processed']))\n",
    "print(pipeline_as(dataset_BPSA[0]['train'][2]['review_text_processed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd7d2b",
   "metadata": {},
   "source": [
    "#### Dataset's (Facebook comentários de página de cachaça)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf7177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pegando os dataset's de comentários do facebook de páginas de cachaças (extração realizada pelo autor via API)\n",
    "list_file = [DIR + file_dataset_facebook + file for file in os.listdir(DIR + file_dataset_facebook) if file.endswith('.csv')] ## pegando os arquivos \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43096ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_facebook = load_dataset(\"csv\", data_files = list_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de3b71",
   "metadata": {},
   "source": [
    "#### Analisando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52648c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quantidade de informações: \" + str(len(dataset_facebook['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85342c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_facebook['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_facebook['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92503da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7cc735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487340bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5a8c855",
   "metadata": {},
   "source": [
    "##### Teste manual com pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60859bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Texto: \" + str(dataset_facebook['train'][55]['message']))\n",
    "print(pipeline_as(dataset_facebook['train'][55]['message']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cd54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Texto: \" + str(dataset_facebook['train'][102]['message']))\n",
    "print(pipeline_as(dataset_facebook['train'][102]['message']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Texto: \" + str(dataset_facebook['train'][86]['message']))\n",
    "print(pipeline_as(dataset_facebook['train'][86]['message']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba783d",
   "metadata": {},
   "source": [
    "#### Dataset's (Twitter comentários de página de cachaça)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dfdbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pegando os dataset's de comentários do twitter de páginas de cachaças (extração realizada pelo autor via API)\n",
    "list_file = [DIR + file_dataset_twitter + file for file in os.listdir(DIR + file_dataset_twitter) if file.endswith('.csv')] ## pegando os arquivos \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007aadd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_twitter = load_dataset(\"csv\", data_files = list_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b03a87",
   "metadata": {},
   "source": [
    "#### Analisando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e65238",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quantidade de informações: \" + str(len(dataset_twitter['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_twitter['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_twitter['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854efd53",
   "metadata": {},
   "source": [
    "##### Teste manual com pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92721309",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 55\n",
    "print(\"Texto: \" + str(dataset_twitter['train'][i]['texto']) + \"\\nResultado (TextBlob): \" + str(dataset_twitter['train'][i]['score'])) \n",
    "print(pipeline_as(dataset_twitter['train'][i]['texto']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58fa1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1000\n",
    "print(\"Texto: \" + str(dataset_twitter['train'][i]['texto']) + \"\\nResultado (TextBlob): \" + str(dataset_twitter['train'][i]['score'])) \n",
    "print(pipeline_as(dataset_twitter['train'][i]['texto']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb93cea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 99\n",
    "print(\"Texto: \" + str(dataset_twitter['train'][i]['texto']) + \"\\nResultado (TextBlob): \" + str(dataset_twitter['train'][i]['score'])) \n",
    "print(pipeline_as(dataset_twitter['train'][i]['texto']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e738fdf",
   "metadata": {},
   "source": [
    "### TextBlob vs Pipeline (Twitter Roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conserta_score(dado):\n",
    "    resultado = dado['score'].split(',')[0]\n",
    "    if resultado == 'Neutro':\n",
    "        resultado = 'Neutral'\n",
    "    elif resultado == 'Positivo':\n",
    "        resultado = 'Positive'\n",
    "    else:\n",
    "        resultado = 'Negative'\n",
    "    dado['score'] = resultado\n",
    "    return dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_twitter_teste = dataset_twitter['train'].map(conserta_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaebd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_twitter_teste[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55effc33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qtd_acerto = 0\n",
    "for dado in dataset_twitter_teste:\n",
    "    resultado = pipeline_as(dado['texto'])\n",
    "    if resultado[0]['label'] == dado['score']:\n",
    "        qtd_acerto += 1\n",
    "print(\"Quantidade de matches de anailise de sentimento: \" + str(qtd_acerto))\n",
    "print(\"Procentagem de acerto: \" + str((qtd_acerto/len(dataset_twitter_teste)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4edf561",
   "metadata": {},
   "source": [
    "### Validando Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f3df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(DIR + 'resultados.json', 'w') as arquivo:\n",
    "    json.dump(resultados, arquivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1861965",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc75f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b82046c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
