{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import create_optimizer\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"../../\"\n",
    "dir_data = \"data/\"\n",
    "dir_result = \"results/\"\n",
    "\n",
    "dataset_arquivo = \"dataset_utlc_movies.json\" \n",
    "\n",
    "model_id = \"neuralmind/bert-base-portuguese-cased\"\n",
    "\n",
    "memoria_cpu = 24 * 1024\n",
    "poct_memoria_cpu = 0.9\n",
    "\n",
    "max_length = 128\n",
    "num_class = 2\n",
    "num_batchs = 16\n",
    "num_epochs = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set MemÃ³ria CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "assert gpus\n",
    "print(\"GPUs identificada!\")\n",
    "for gpu in gpus:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpu,\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=memoria_cpu * poct_memoria_cpu)]\n",
    "    )\n",
    "    # tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_json(f\"{dir}{dir_data}{dataset_arquivo}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixando Modelo e Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_id, num_labels=num_class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizando Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniza_dataset(data):\n",
    "    return tokenizer(data[\"text\"], padding=True, return_tensors=\"tf\", max_length=max_length, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(tokeniza_dataset, batched=True, batch_size=num_batchs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividindo Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by(dataset: Dataset) -> list[Dataset]:\n",
    "    lista_grupos = []\n",
    "    for grupo in set(dataset[\"group\"]):\n",
    "        lista_grupos.append(dataset.filter(lambda x: x[\"group\"]==grupo))\n",
    "    return lista_grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_dataset = group_by(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_agrupamento(lista_dataset: list[Dataset]) -> int:\n",
    "    soma = 0\n",
    "    for dataset in lista_dataset:\n",
    "        soma += len(dataset)\n",
    "    return round(soma/len(lista_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = (get_mean_agrupamento(lista_dataset) // num_batchs) * num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, _ = create_optimizer(\n",
    "            init_lr=2e-5,\n",
    "            num_train_steps=num_train_steps,\n",
    "            weight_decay_rate=0.01,\n",
    "            num_warmup_steps=0,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_colunas(lista_dataset: list[Dataset], colunas: list[str] = [\"text\", \"group\"]):\n",
    "    for index, dataset in enumerate(lista_dataset):\n",
    "        lista_dataset[index] = dataset.remove_columns(colunas)\n",
    "    return lista_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_dataset = remove_colunas(lista_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_loss(y_true, y_pred):\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    return loss(y_true, y_pred.logits).numpy()\n",
    "\n",
    "def func_acc(y_true, y_pred):\n",
    "    y_pred = np.argmax(y_pred.logits, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    acc = tf.keras.metrics.Accuracy()\n",
    "    acc.update_state(y_true, y_pred)\n",
    "    return acc.result().numpy()\n",
    "\n",
    "def func_precision(y_true, y_pred):\n",
    "    y_pred = np.argmax(y_pred.logits, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    precision.update_state(y_true, y_pred)\n",
    "    return precision.result().numpy()\n",
    "\n",
    "def func_recall(y_true, y_pred):\n",
    "    y_pred = np.argmax(y_pred.logits, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    recall.update_state(y_true, y_pred)\n",
    "    return recall.result().numpy()\n",
    "\n",
    "def func_f1(y_true, y_pred):\n",
    "    precision = func_precision(y_true, y_pred)\n",
    "    recall = func_recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def func_roc_auc(y_true, y_pred):\n",
    "    y_pred = np.argmax(y_pred.logits, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    roc_auc = tf.keras.metrics.AUC()\n",
    "    roc_auc.update_state(y_true, y_pred)\n",
    "    return roc_auc.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Treino\n",
    "dataset_treino = lista_dataset[:-1]\n",
    "dataset_treino = concatenate_datasets(dataset_treino)\n",
    "tf_dataset_treino = model.prepare_tf_dataset(dataset_treino, batch_size=num_batchs, shuffle=True, tokenizer=tokenizer)\n",
    "\n",
    "## Teste\n",
    "tf_dataset_teste = model.prepare_tf_dataset(lista_dataset[-1], batch_size=num_batchs, shuffle=False, tokenizer=tokenizer)\n",
    "\n",
    "## Modelo\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy()\n",
    "    ]\n",
    ")\n",
    "\n",
    "## Treinamento\n",
    "model.fit(tf_dataset_treino, batch_size=num_batchs, epochs=num_epochs, use_multiprocessing=True)\n",
    "\n",
    "## Teste\n",
    "y_pred = model.predict(tf_dataset_teste, batch_size=num_batchs, use_multiprocessing=True)\n",
    "\n",
    "## Metricas\n",
    "acc = func_acc(lista_dataset[-1][\"labels\"], y_pred)\n",
    "precision = func_precision(lista_dataset[-1][\"labels\"], y_pred)\n",
    "recall = func_recall(lista_dataset[-1][\"labels\"], y_pred)\n",
    "f1 = func_f1(lista_dataset[-1][\"labels\"], y_pred)\n",
    "loss = func_loss(lista_dataset[-1][\"labels\"], y_pred)\n",
    "roc_auc = func_roc_auc(lista_dataset[-1][\"labels\"], y_pred)\n",
    "####################\n",
    "\n",
    "resultado = dict({\n",
    "    \"loss\" : float(loss),\n",
    "    \"accuracy\" : float(acc),\n",
    "    \"precision\" : float(precision),\n",
    "    \"recall\" : float(recall),\n",
    "    \"f1\" : float(f1),\n",
    "    \"roc_auc\" : float(roc_auc)\n",
    "})\n",
    "\n",
    "with open(f\"{dir}{dir_result}{dataset_arquivo}\", \"w\") as arquivo:\n",
    "    json.dump(resultado, arquivo, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c6567b4a0b385df8abdfd302ad3b39b45fd5aade3f2daa246533e4b99d5affc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
