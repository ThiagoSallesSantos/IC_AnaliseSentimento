{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from datasets import Dataset\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"../../\"\n",
    "dir_data = \"data/\"\n",
    "\n",
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "\n",
    "qtd_classes = 2\n",
    "\n",
    "dir_resultado = \"results/\"\n",
    "dir_resultado_espec = \"baselines/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo Dataset's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4644021550a575e4\n",
      "Found cached dataset json (/home/thiago/.cache/huggingface/datasets/json/default-4644021550a575e4/0.0.0)\n",
      "Using custom data configuration default-be9c838f6c2f2531\n",
      "Found cached dataset json (/home/thiago/.cache/huggingface/datasets/json/default-be9c838f6c2f2531/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "lista_datasets = []\n",
    "for arquivo in os.listdir(f\"{dir}{dir_data}\"):\n",
    "    if arquivo.endswith(\".json\"):\n",
    "        lista_datasets.append([arquivo[8:-5], Dataset.from_json(f\"{dir}{dir_data}{arquivo}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['concatenate',\n",
       "  Dataset({\n",
       "      features: ['text', 'labels', 'labels_int'],\n",
       "      num_rows: 2386146\n",
       "  })],\n",
       " ['buscape',\n",
       "  Dataset({\n",
       "      features: ['text', 'labels', 'group', 'labels_int'],\n",
       "      num_rows: 73626\n",
       "  })]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegando Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 23:08:44.611542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 23:08:44.616826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 23:08:44.616938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 23:08:44.616990: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1953] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 8.6. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2023-10-27 23:08:44.617610: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-27 23:08:44.618172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 23:08:44.618277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 23:08:44.618369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 23:08:44.618419: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1953] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 8.6. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2023-10-27 23:08:45.794603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 23:08:45.794770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 23:08:45.794869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 23:08:45.794941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22202 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "All model checkpoint layers were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFXLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de Perda (CategoricalCrossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_loss(y_true, y_pred):\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    return loss(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica de Acurácia Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_acc(y_true, y_pred):\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    acc = tf.keras.metrics.Accuracy()\n",
    "    acc.update_state(y_true, y_pred)\n",
    "    return acc.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica de Precisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_precision(y_true, y_pred):\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    precision.update_state(y_true, y_pred)\n",
    "    return precision.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica de Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_recall(y_true, y_pred):\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    recall.update_state(y_true, y_pred)\n",
    "    return recall.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica de F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_f1(y_true, y_pred):\n",
    "    precision = func_precision(y_true, y_pred)\n",
    "    recall = func_recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica de ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_roc_auc(y_true, y_pred):\n",
    "    roc_auc = tf.keras.metrics.AUC()\n",
    "    roc_auc.update_state(y_true, y_pred)\n",
    "    return roc_auc.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando dataset's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E-Commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76735629a734409cab3e09cc74a62420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2387 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/thiago/IC_AnaliseSentimento/src/models_code/Modelo_XLM-RoBERTA_baselines.ipynb Célula 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.18.0.1/home/thiago/IC_AnaliseSentimento/src/models_code/Modelo_XLM-RoBERTA_baselines.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(lista_datasets):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.18.0.1/home/thiago/IC_AnaliseSentimento/src/models_code/Modelo_XLM-RoBERTA_baselines.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     lista_datasets[index][\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mfilter(\u001b[39mlambda\u001b[39;49;00m dado: dado[\u001b[39m\"\u001b[39;49m\u001b[39mgroup\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m==\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/IC_AnaliseSentimento/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:551\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    545\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    546\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    547\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    548\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    549\u001b[0m }\n\u001b[1;32m    550\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    552\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    553\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/IC_AnaliseSentimento/venv/lib/python3.9/site-packages/datasets/fingerprint.py:480\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    478\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    482\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/IC_AnaliseSentimento/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:3100\u001b[0m, in \u001b[0;36mDataset.filter\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3097\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   3098\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m-> 3100\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m   3101\u001b[0m     function\u001b[39m=\u001b[39;49mpartial(\n\u001b[1;32m   3102\u001b[0m         get_indices_from_mask_function, function, batched, with_indices, input_columns, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices\n\u001b[1;32m   3103\u001b[0m     ),\n\u001b[1;32m   3104\u001b[0m     with_indices\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   3105\u001b[0m     features\u001b[39m=\u001b[39;49mFeatures({\u001b[39m\"\u001b[39;49m\u001b[39mindices\u001b[39;49m\u001b[39m\"\u001b[39;49m: Value(\u001b[39m\"\u001b[39;49m\u001b[39muint64\u001b[39;49m\u001b[39m\"\u001b[39;49m)}),\n\u001b[1;32m   3106\u001b[0m     batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   3107\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   3108\u001b[0m     remove_columns\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumn_names,\n\u001b[1;32m   3109\u001b[0m     keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   3110\u001b[0m     load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   3111\u001b[0m     cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   3112\u001b[0m     writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   3113\u001b[0m     fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   3114\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   3115\u001b[0m     suffix_template\u001b[39m=\u001b[39;49msuffix_template,\n\u001b[1;32m   3116\u001b[0m     new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   3117\u001b[0m     input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   3118\u001b[0m     desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   3119\u001b[0m )\n\u001b[1;32m   3120\u001b[0m new_dataset \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m)\n\u001b[1;32m   3121\u001b[0m new_dataset\u001b[39m.\u001b[39m_indices \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mdata\n",
      "File \u001b[0;32m~/IC_AnaliseSentimento/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:2572\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2569\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   2571\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2572\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   2573\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   2574\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   2575\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   2576\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   2577\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   2578\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2579\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   2580\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   2581\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   2582\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   2583\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   2584\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   2585\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   2586\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   2587\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   2588\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   2589\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   2590\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   2591\u001b[0m     )\n\u001b[1;32m   2592\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2594\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m~/IC_AnaliseSentimento/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:584\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    583\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    585\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    586\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    587\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/IC_AnaliseSentimento/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:551\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    545\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    546\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    547\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    548\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    549\u001b[0m }\n\u001b[1;32m    550\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    552\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    553\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/IC_AnaliseSentimento/venv/lib/python3.9/site-packages/datasets/fingerprint.py:480\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    478\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    482\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/IC_AnaliseSentimento/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:2968\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2964\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   2965\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(input_dataset\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   2966\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   2967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2968\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   2969\u001b[0m         batch,\n\u001b[1;32m   2970\u001b[0m         indices,\n\u001b[1;32m   2971\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(input_dataset\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2972\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   2973\u001b[0m     )\n\u001b[1;32m   2974\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   2975\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   2976\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2977\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/IC_AnaliseSentimento/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:2852\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2850\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   2851\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 2852\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   2853\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2854\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[1;32m   2855\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n",
      "File \u001b[0;32m~/IC_AnaliseSentimento/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:2532\u001b[0m, in \u001b[0;36mDataset.map.<locals>.decorate.<locals>.decorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2528\u001b[0m decorated_item \u001b[39m=\u001b[39m (\n\u001b[1;32m   2529\u001b[0m     Example(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched \u001b[39melse\u001b[39;00m Batch(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n\u001b[1;32m   2530\u001b[0m )\n\u001b[1;32m   2531\u001b[0m \u001b[39m# Use the LazyDict internally, while mapping the function\u001b[39;00m\n\u001b[0;32m-> 2532\u001b[0m result \u001b[39m=\u001b[39m f(decorated_item, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2533\u001b[0m \u001b[39m# Return a standard dict\u001b[39;00m\n\u001b[1;32m   2534\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mdata \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, LazyDict) \u001b[39melse\u001b[39;00m result\n",
      "File \u001b[0;32m~/IC_AnaliseSentimento/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:5257\u001b[0m, in \u001b[0;36mget_indices_from_mask_function\u001b[0;34m(function, batched, with_indices, input_columns, indices_mapping, *args, **fn_kwargs)\u001b[0m\n\u001b[1;32m   5254\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_examples):\n\u001b[1;32m   5255\u001b[0m         example \u001b[39m=\u001b[39m {key: batch[key][i] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m batch}\n\u001b[1;32m   5256\u001b[0m         mask\u001b[39m.\u001b[39mappend(\n\u001b[0;32m-> 5257\u001b[0m             function(example, indices[i], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_kwargs) \u001b[39mif\u001b[39;00m with_indices \u001b[39melse\u001b[39;00m function(example, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   5258\u001b[0m         )\n\u001b[1;32m   5259\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5260\u001b[0m     \u001b[39m# inputs is a list of columns\u001b[39;00m\n\u001b[1;32m   5261\u001b[0m     columns: List[List] \u001b[39m=\u001b[39m inputs\n",
      "\u001b[1;32m/home/thiago/IC_AnaliseSentimento/src/models_code/Modelo_XLM-RoBERTA_baselines.ipynb Célula 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.18.0.1/home/thiago/IC_AnaliseSentimento/src/models_code/Modelo_XLM-RoBERTA_baselines.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(lista_datasets):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.18.0.1/home/thiago/IC_AnaliseSentimento/src/models_code/Modelo_XLM-RoBERTA_baselines.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     lista_datasets[index][\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mfilter(\u001b[39mlambda\u001b[39;00m dado: dado[\u001b[39m\"\u001b[39;49m\u001b[39mgroup\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m==\u001b[39m\u001b[39m10\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'group'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mO Kernel falhou ao executar o código na célula atual ou em uma célula anterior. Examine o código nas células para identificar uma possível causa da falha. Clique <a href=\"https://aka.ms/vscodeJupyterKernelCrash\">aqui</a> para obter mais informações. Consulte o <a href='command:jupyter.viewOutput'>log</a> do Jupyter para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "for index, data in enumerate(lista_datasets):\n",
    "    lista_datasets[index][1] = data[1].filter(lambda dado: dado[\"group\"]==10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter ASBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_neutro(data):\n",
    "    data[\"labels\"].pop(1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, data in enumerate(lista_datasets):\n",
    "    lista_datasets[index][1] = data[1].filter(lambda dado: dado[\"group\"]==\"test\")\n",
    "    if qtd_classes == 2:\n",
    "        lista_datasets[index][1] = data[1].filter(lambda dado: dado[\"labels_int\"] != 1)\n",
    "        lista_datasets[index][1] = lista_datasets[index][1].map(remove_neutro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizador(dataset) -> list:\n",
    "    lista_texto_tokenizado = []\n",
    "    for data in dataset:\n",
    "        lista_texto_tokenizado.append(tokenizer(data[\"text\"], return_tensors='tf'))\n",
    "    return lista_texto_tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nome_dataset, dataset in lista_datasets:\n",
    "    lista_resultados = []\n",
    "    lista_texto_tokenizado = tokenizador(dataset)\n",
    "    for texto_tokenizado in lista_texto_tokenizado:\n",
    "        resultado = model(texto_tokenizado)\n",
    "        lista_resultados.append(resultado.logits)\n",
    "    if qtd_classes == 2:\n",
    "        for index, data in enumerate(lista_resultados):\n",
    "            lista_resultados[index] = np.delete(data, 1)\n",
    "\n",
    "    if qtd_classes == 3:\n",
    "        resultados = list([dict({\n",
    "            \"accuracy\" : float(accuracy_score(np.argmax(dataset[\"labels\"], axis=1), np.asarray(np.argmax(lista_resultados,axis=-1)).ravel())),\n",
    "            \"precision\" : float(precision_score(np.argmax(dataset[\"labels\"], axis=1), np.asarray(np.argmax(lista_resultados,axis=-1)).ravel(), average='macro')),\n",
    "            \"recall\" : float(recall_score(np.argmax(dataset[\"labels\"], axis=1), np.asarray(np.argmax(lista_resultados,axis=-1)).ravel(), average='weighted')),\n",
    "            \"f1\" : float(f1_score(np.argmax(dataset[\"labels\"], axis=1), np.asarray(np.argmax(lista_resultados,axis=-1)).ravel(), average='weighted'))\n",
    "        })])\n",
    "        \n",
    "    else:\n",
    "        resultados = list([dict({\n",
    "            \"loss\" : float(func_loss(dataset[\"labels\"], softmax(lista_resultados))),\n",
    "            \"accuracy\" : float(func_acc(dataset[\"labels\"], softmax(lista_resultados))),\n",
    "            \"precision\" : float(func_precision(dataset[\"labels\"], softmax(lista_resultados))),\n",
    "            \"recall\" : float(func_recall(dataset[\"labels\"], softmax(lista_resultados))),\n",
    "            \"f1\" : float(func_f1(dataset[\"labels\"], softmax(lista_resultados))),\n",
    "            \"roc_auc\" : float(func_roc_auc(np.argmax(dataset[\"labels\"], axis=1), np.argmax(lista_resultados, axis=1)))\n",
    "        })])\n",
    "\n",
    "    with open(f\"{dir}{dir_resultado}{dir_resultado_espec}dataset_{nome_dataset}_RoBERTa.json\", \"w\") as arquivo:\n",
    "        json.dump(resultados, arquivo, indent=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
