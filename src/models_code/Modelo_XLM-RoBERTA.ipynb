{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from datasets import Dataset\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"../../\"\n",
    "dir_data = \"data/\"\n",
    "\n",
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "\n",
    "qtd_classes = 3\n",
    "\n",
    "dir_resultado = \"results/\"\n",
    "dir_resultado_espec = \"baselines/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo Dataset's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7ef50e8b713c5403\n",
      "Found cached dataset json (/home/thiago/.cache/huggingface/datasets/json/default-7ef50e8b713c5403/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "lista_datasets = []\n",
    "for arquivo in os.listdir(f\"{dir}{dir_data}\"):\n",
    "    if arquivo.endswith(\".json\"):\n",
    "        lista_datasets.append([arquivo[8:-5], Dataset.from_json(f\"{dir}{dir_data}{arquivo}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['twitter_ASBR',\n",
       "  Dataset({\n",
       "      features: ['labels', 'group', 'text', 'labels_int'],\n",
       "      num_rows: 14788\n",
       "  })]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegando Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 11:03:24.198423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 11:03:24.203773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 11:03:24.203892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 11:03:24.203943: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1953] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 8.6. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2023-10-27 11:03:24.204330: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-27 11:03:24.205375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 11:03:24.205483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 11:03:24.205577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 11:03:24.205627: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1953] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 8.6. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2023-10-27 11:03:25.352837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 11:03:25.352993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 11:03:25.353089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-27 11:03:25.353159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22199 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "All model checkpoint layers were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFXLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de Perda (CategoricalCrossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_loss(y_true, y_pred):\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    return loss(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica de Acurácia Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_acc(y_true, y_pred):\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    acc = tf.keras.metrics.Accuracy()\n",
    "    acc.update_state(y_true, y_pred)\n",
    "    return acc.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica de Precisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_precision(y_true, y_pred):\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    precision.update_state(y_true, y_pred)\n",
    "    return precision.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica de Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_recall(y_true, y_pred):\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    recall.update_state(y_true, y_pred)\n",
    "    return recall.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica de F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_f1(y_true, y_pred):\n",
    "    precision = func_precision(y_true, y_pred)\n",
    "    recall = func_recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica de ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_roc_auc(y_true, y_pred):\n",
    "    roc_auc = tf.keras.metrics.AUC()\n",
    "    roc_auc.update_state(y_true, y_pred.logits)\n",
    "    return roc_auc.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando dataset's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E-Commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, data in enumerate(lista_datasets):\n",
    "    lista_datasets[index][1] = data[1].filter(lambda dado: dado[\"group\"]==10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter ASBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_neutro(data):\n",
    "    data[\"labels\"].pop(1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/thiago/.cache/huggingface/datasets/json/default-7ef50e8b713c5403/0.0.0/cache-4c903f1c2112c05b.arrow\n"
     ]
    }
   ],
   "source": [
    "for index, data in enumerate(lista_datasets):\n",
    "    lista_datasets[index][1] = data[1].filter(lambda dado: dado[\"group\"]==\"test\")\n",
    "    if qtd_classes == 2:\n",
    "        lista_datasets[index][1] = data[1].filter(lambda dado: dado[\"labels_int\"] != 1)\n",
    "        lista_datasets[index][1] = lista_datasets[index][1].map(remove_neutro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizador(dataset) -> list:\n",
    "    lista_texto_tokenizado = []\n",
    "    for data in dataset:\n",
    "        lista_texto_tokenizado.append(tokenizer(data[\"text\"], return_tensors='tf'))\n",
    "    return lista_texto_tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nome_dataset, dataset in lista_datasets:\n",
    "    lista_resultados = []\n",
    "    lista_texto_tokenizado = tokenizador(dataset)\n",
    "    for texto_tokenizado in lista_texto_tokenizado:\n",
    "        resultado = model(texto_tokenizado)\n",
    "        lista_resultados.append(resultado.logits)\n",
    "    if qtd_classes == 2:\n",
    "        for index, data in enumerate(lista_resultados):\n",
    "            lista_resultados[index] = np.delete(data, 1)\n",
    "\n",
    "    if qtd_classes == 3:\n",
    "        resultados = list([dict({\n",
    "            \"accuracy\" : float(accuracy_score(np.argmax(dataset[\"labels\"], axis=1), np.asarray(np.argmax(lista_resultados,axis=-1)).ravel())),\n",
    "            \"precision\" : float(precision_score(np.argmax(dataset[\"labels\"], axis=1), np.asarray(np.argmax(lista_resultados,axis=-1)).ravel(), average='micro')),\n",
    "            \"recall\" : float(recall_score(np.argmax(dataset[\"labels\"], axis=1), np.asarray(np.argmax(lista_resultados,axis=-1)).ravel(), average='micro')),\n",
    "            \"f1\" : float(f1_score(np.argmax(dataset[\"labels\"], axis=1), np.asarray(np.argmax(lista_resultados,axis=-1)).ravel(), average='micro'))\n",
    "        })])\n",
    "        \n",
    "    else:\n",
    "        resultados = list([dict({\n",
    "            \"loss\" : float(func_loss(dataset[\"labels\"], softmax(lista_resultados))),\n",
    "            \"accuracy\" : float(func_acc(dataset[\"labels\"], softmax(lista_resultados))),\n",
    "            \"precision\" : float(func_precision(dataset[\"labels\"], softmax(lista_resultados))),\n",
    "            \"recall\" : float(func_recall(dataset[\"labels\"], softmax(lista_resultados))),\n",
    "            \"f1\" : float(func_f1(dataset[\"labels\"], softmax(lista_resultados)))\n",
    "        })])\n",
    "\n",
    "    with open(f\"{dir}{dir_resultado}{dir_resultado_espec}dataset_{nome_dataset}_RoBERTa_3_classes.json\", \"w\") as arquivo:\n",
    "        json.dump(resultados, arquivo, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [2],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(np.argmax(softmax(lista_resultados), axis=-1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
