{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from datasets import Dataset\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"../../\"\n",
    "dir_data = \"data/datasets/\"\n",
    "dataset_nome = \"twitter\"\n",
    "dir_data_dataset = f\"dataset_{dataset_nome}/\"\n",
    "\n",
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "\n",
    "dir_resultado = f\"results/cachaca/{dataset_nome}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo Dataset's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b00294bf64c354e0\n",
      "Found cached dataset json (/home/thiago/.cache/huggingface/datasets/json/default-b00294bf64c354e0/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "dataset = None\n",
    "for arquivo in os.listdir(f\"{dir}{dir_data}{dir_data_dataset}\"):\n",
    "    if arquivo.endswith(\".json\"):\n",
    "        dataset = Dataset.from_json(f\"{dir}{dir_data}{dir_data_dataset}{arquivo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'conversation_id', 'text', 'created_at', 'data_coleta', 'repetido', 'index', 'labels', 'labels_int', 'tweet_link'],\n",
       "    num_rows: 17766\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegando Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 12:22:59.379896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 12:22:59.385369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 12:22:59.385493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 12:22:59.385545: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1953] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 8.6. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2023-11-14 12:22:59.385978: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-14 12:22:59.386558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 12:22:59.386662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 12:22:59.386752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 12:22:59.386801: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1953] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 8.6. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2023-11-14 12:23:00.575625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 12:23:00.575797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 12:23:00.575898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-14 12:23:00.575972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21986 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "All model checkpoint layers were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFXLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de Perda (CategoricalCrossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_loss(y_true, y_pred):\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    return loss(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica de Acurácia Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_acc(y_true, y_pred):\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    acc = tf.keras.metrics.Accuracy()\n",
    "    acc.update_state(y_true, y_pred)\n",
    "    return acc.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica de Precisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_precision(y_true, y_pred):\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    precision.update_state(y_true, y_pred)\n",
    "    return precision.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica de Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_recall(y_true, y_pred):\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    recall.update_state(y_true, y_pred)\n",
    "    return recall.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica de F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_f1(y_true, y_pred):\n",
    "    precision = func_precision(y_true, y_pred)\n",
    "    recall = func_recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica de ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_roc_auc(y_true, y_pred):\n",
    "    roc_auc = tf.keras.metrics.AUC()\n",
    "    roc_auc.update_state(y_true, y_pred.logits)\n",
    "    return roc_auc.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando dataset's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/thiago/.cache/huggingface/datasets/json/default-b00294bf64c354e0/0.0.0/cache-11f02f905290c4be.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_rotulado = dataset.filter(lambda data: data[\"labels_int\"] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_rotulado_sem_neutro = dataset_rotulado.filter(lambda data: data[\"labels_int\"] != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_neutro(data):\n",
    "    data[\"labels\"].pop(1)\n",
    "    return data\n",
    "\n",
    "dataset = dataset_rotulado_sem_neutro.map(remove_neutro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_rotulado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'conversation_id', 'text', 'created_at', 'data_coleta', 'repetido', 'index', 'labels', 'labels_int', 'tweet_link'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_texto_tokenizado = []\n",
    "for data in dataset:\n",
    "    lista_texto_tokenizado.append(tokenizer(data[\"text\"], return_tensors='tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_resultados = []\n",
    "for texto_tokenizado in lista_texto_tokenizado:\n",
    "    resultado = model(texto_tokenizado)\n",
    "    lista_resultados.append(resultado.logits)\n",
    "\n",
    "# for index, data in enumerate(lista_resultados):\n",
    "#     lista_resultados[index] = np.delete(data, 1)\n",
    "\n",
    "resultados = list([dict({\n",
    "    \"accuracy\" : float(accuracy_score(np.argmax(dataset[\"labels\"], axis=1), np.asarray(np.argmax(lista_resultados,axis=-1)).ravel())),\n",
    "    \"precision\" : float(precision_score(np.argmax(dataset[\"labels\"], axis=1), np.asarray(np.argmax(lista_resultados,axis=-1)).ravel(), average='macro')),\n",
    "    \"recall\" : float(recall_score(np.argmax(dataset[\"labels\"], axis=1), np.asarray(np.argmax(lista_resultados,axis=-1)).ravel(), average='macro')),\n",
    "    \"f1\" : float(f1_score(np.argmax(dataset[\"labels\"], axis=1), np.asarray(np.argmax(lista_resultados,axis=-1)).ravel(), average='macro'))\n",
    "})])\n",
    "\n",
    "# resultados = list([dict({\n",
    "#     \"loss\" : float(func_loss(dataset[\"labels\"], softmax(lista_resultados))),\n",
    "#     \"accuracy\" : float(func_acc(dataset[\"labels\"], softmax(lista_resultados))),\n",
    "#     \"precision\" : float(func_precision(dataset[\"labels\"], softmax(lista_resultados))),\n",
    "#     \"recall\" : float(func_recall(dataset[\"labels\"], softmax(lista_resultados))),\n",
    "#     \"f1\" : float(func_f1(dataset[\"labels\"], softmax(lista_resultados)))\n",
    "# })])\n",
    "\n",
    "with open(f\"{dir}{dir_resultado}resultados_{dataset_nome}_RoBERTa_3_classes.json\", \"w\") as arquivo:\n",
    "    json.dump(resultados, arquivo, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(lista_resultados) == len(dataset):\n",
    "    lista_resultado_predito = []\n",
    "    for index, dado in enumerate(dataset):\n",
    "        real = np.argmax(dado[\"labels\"])\n",
    "        predito = np.argmax(lista_resultados[index])\n",
    "        lista_resultado_predito.append(dict({\n",
    "            \"texto\": dado[\"text\"],\n",
    "            \"real\" : str(real),\n",
    "            \"predito\" : str(predito)\n",
    "        }))\n",
    "    with open(f\"{dir}/results/teste/{dataset_nome}/resultados_modelo_twitter-xlm-roberta_2.json\", \"w\") as arquivo:\n",
    "        json.dump(lista_resultado_predito, arquivo, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
